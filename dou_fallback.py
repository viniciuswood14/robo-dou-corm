# Nome do arquivo: dou_fallback.py
# Vers√£o: 8.0 (Estilo Ro-DOU - Busca P√∫blica Bruta)

import httpx
from bs4 import BeautifulSoup
from datetime import datetime
import asyncio
import random
import re
from typing import List, Dict

# URL que o Ro-DOU e o site oficial usam para pesquisar
SEARCH_URL = "https://www.in.gov.br/consulta/-/buscar/dou"

async def buscar_termo_bruto(termo: str, data_pt: str, secao_param: str) -> List[Dict]:
    """
    Realiza a busca no portal e extrai links usando regex e soup.
    """
    results = []
    
    # Par√¢metros exatos da busca avan√ßada do DOU
    params = {
        "q": f'"{termo}"', # Aspas para exatid√£o (opcional, mas ajuda a filtrar lixo)
        "s": secao_param,  # do1, do2, doe
        "exact": "true",
        "dt": data_pt,     # Data inicial
        "dtEnd": data_pt,  # Data final
        "sortType": "0"    # Relev√¢ncia
    }
    
    # Headers de navegador padr√£o
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }

    async with httpx.AsyncClient(timeout=30, follow_redirects=True) as client:
        try:
            # Pequeno delay aleat√≥rio para n√£o parecer ataque DDoS
            await asyncio.sleep(random.uniform(0.2, 0.5))
            
            resp = await client.get(SEARCH_URL, params=params, headers=headers)
            
            if resp.status_code != 200:
                print(f"‚ùå [ERRO HTTP] Busca '{termo}': {resp.status_code}")
                return []

            soup = BeautifulSoup(resp.text, "html.parser")
            
            # --- ESTRAT√âGIA ROBUSTA DE EXTRA√á√ÉO ---
            # 1. Tenta encontrar links de mat√©rias pelo padr√£o de URL
            # Padr√£o: /web/dou/-/titulo-da-materia-id
            links_encontrados = soup.find_all("a", href=re.compile(r"/web/dou/-/"))
            
            processed_urls = set()

            for tag in links_encontrados:
                href = tag.get("href")
                if not href: continue
                
                # Monta URL absoluta
                full_link = f"https://www.in.gov.br{href}" if href.startswith("/") else href
                
                # Remove duplicatas
                if full_link in processed_urls: continue
                processed_urls.add(full_link)

                # Extrai T√≠tulo
                # Limpa espa√ßos e quebras de linha
                title = " ".join(tag.get_text().split())
                
                # Valida√ß√£o b√°sica de t√≠tulo (evita links de √≠cones vazios)
                if len(title) < 5:
                    title = tag.get("title", "")
                    if len(title) < 5: continue

                # Tenta extrair um Resumo (Snippet)
                # Geralmente o resumo est√° num <p> ou <div> pr√≥ximo ao link
                abstract = ""
                # Procura um container pai pr√≥ximo (ex: o card do resultado)
                card = tag.find_parent("div", class_=re.compile(r"(result|item|search)"))
                if card:
                    # Pega o texto do card, remove o t√≠tulo para sobrar o resumo
                    full_text = " ".join(card.get_text().split())
                    abstract = full_text.replace(title, "").strip()[:400] # Pega 400 chars
                
                # Se n√£o achou card, usa o pr√≥prio t√≠tulo como resumo
                if not abstract: abstract = "Conte√∫do obtido via busca p√∫blica."

                results.append({
                    "organ": "DOU P√∫blico (Busca)",
                    "type": "Resultado",
                    "summary": title,
                    "raw": f"{title}\n{abstract}\nLink: {full_link}",
                    "relevance_reason": f"Termo encontrado: '{termo}'",
                    "section": secao_param.upper(),
                    "link": full_link
                })
            
            if results:
                print(f"   ‚úÖ '{termo}': {len(results)} resultados.", flush=True)

        except Exception as e:
            print(f"‚ùå [EXCE√á√ÉO] '{termo}': {e}", flush=True)

    return results

async def executar_fallback(data_iso: str, keywords: List[str]) -> List[Dict]:
    # 1. Configura Data
    try:
        dt = datetime.strptime(data_iso.strip(), "%Y-%m-%d")
        data_pt = dt.strftime("%d-%m-%Y") # Busca exige DD-MM-YYYY
    except Exception as e:
        print(f"‚ùå [FALLBACK] Erro data: {e}", flush=True)
        return []

    # 2. Lista de Keywords (Focada e Otimizada para Busca)
    # Na busca, menos √© mais. Termos muito gen√©ricos como "Lei" trazem lixo.
    # Focamos nas UGs e Termos Compostos.
    termos_criticos = [
        '"Marinha do Brasil"', # Aspas for√ßam frase exata
        '"Comando da Marinha"',
        '"PROSUB"',
        '"Amazul"',
        "52131", # UG MB
        "52000", # UG MD
        '"Or√ßamento Fiscal"',
        '"Cr√©dito Suplementar"',
        '"Remanejamento"',
        '"Minist√©rio da Defesa"',
        '"For√ßas Armadas"',
        '"Autoridade Mar√≠tima"',
        '"Programa Nuclear"',
        '"Amaz√¥nia Azul"',
        '"Plano Plurianual"',
        '"Movimenta√ß√£o e empenho"'
    ]
    
    # Adiciona keywords do usu√°rio (aspas se tiver espa√ßo)
    for k in keywords:
        k = k.strip()
        if " " in k and '"' not in k:
            termos_criticos.append(f'"{k}"')
        else:
            termos_criticos.append(k)
            
    # Remove duplicatas
    lista_busca = list(set(termos_criticos))

    print(f"--- [FALLBACK v8.0] Iniciando BUSCA P√öBLICA para {data_pt} ---", flush=True)
    print(f"    Termos a pesquisar: {len(lista_busca)}", flush=True)

    # 3. Dispara as buscas
    # Se√ß√£o 1 (do1) √© a priorit√°ria para atos normativos e or√ßamento
    # Se√ß√£o 2 (do2) √© pessoal (opcional, pode descomentar se quiser)
    secoes = ["do1"] 
    
    all_tasks = []
    for kw in lista_busca:
        for sec in secoes:
            all_tasks.append(buscar_termo_bruto(kw, data_pt, sec))
    
    # Executa em paralelo
    # (O in.gov.br aguenta bem, mas o render pode ter limite de conex√µes)
    resultados_matrix = await asyncio.gather(*all_tasks)
    
    # 4. Consolida e Deduplica
    final_pubs = []
    seen_links = set()
    
    for lista in resultados_matrix:
        for item in lista:
            if item['link'] not in seen_links:
                final_pubs.append(item)
                seen_links.add(item['link'])
    
    print(f"üìä [FIM FALLBACK] Total Final: {len(final_pubs)} mat√©rias √∫nicas.", flush=True)
    return final_pubs
